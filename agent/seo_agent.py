# Standard library imports
import json
import os
import pickle
from collections import defaultdict
from typing import Dict, Any, List, Optional
from datetime import datetime
import logging

# Third-party imports
from langchain.agents import Tool, initialize_agent
from langchain_openai import ChatOpenAI

# Local imports
from modules.crawler import WebsiteCrawler
from modules.keyword_research import KeywordResearch
from modules.content_generator import ContentGenerator
from modules.seo_optimizer import SEOOptimizer

class SEOAgent:
    """
    Agent that orchestrates SEO analysis, content generation, and optimization.
    
    Attributes:
        llm: Language model for content generation
        crawler: Website crawler instance
        keyword_research: Keyword research tool
        content_generator: Content generation tool
        seo_optimizer: SEO optimization tool
        cache_dir: Directory for caching analysis results
    """
    
    def __init__(self, access_id: str, secret_key: str) -> None:
        """
        Initialize SEO agent with required credentials and tools.
        
        Args:
            access_id: API access ID for keyword research
            secret_key: API secret key for keyword research
        """
        self.logger = logging.getLogger(__name__)
        self.logger.info("Initializing SEO Agent...")
        
        try:
            self.llm = ChatOpenAI(
                api_key=os.getenv('OPENAI_API_KEY'),
                temperature=0.7,
                model="gpt-4"
            )
            self.logger.info("LLM initialized successfully")
            
            # Initialize components
            self.crawler = None
            self.keyword_research = KeywordResearch(access_id, secret_key)
            self.content_generator = ContentGenerator(self.llm)
            self.seo_optimizer = SEOOptimizer()
            self.cache_dir = "cache"
            
            os.makedirs(self.cache_dir, exist_ok=True)
            
            # Setup tools with error handling
            self.logger.info("Setting up tools...")
            self.tools = self._setup_tools()
            
            # Initialize agent with warning suppression
            import warnings
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                self.agent = self._initialize_agent()
            
            self.logger.info("SEO Agent initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Error initializing SEO Agent: {str(e)}")
            self.logger.error("Stack trace:", exc_info=True)
            raise

    def _initialize_agent(self) -> Any:
        """Initialize the LangChain agent with tools."""
        return initialize_agent(
            self.tools,
            self.llm,
            agent="zero-shot-react-description",
            verbose=True
        )

    def _setup_tools(self) -> List[Tool]:
        """Set up the available tools for the agent."""
        return [
            Tool(
                name="Crawler",
                func=self.crawl_website,
                description="Crawls website to analyze structure and content"
            ),
            Tool(
                name="KeywordResearch",
                func=self.research_keywords,
                description="Finds relevant keywords for the website"
            ),
            Tool(
                name="ContentGenerator",
                func=self.generate_content,
                description="Generates SEO-optimized content"
            ),
            Tool(
                name="SEOOptimizer",
                func=self.optimize_content,
                description="Applies SEO best practices to content"
            )
        ]

    def _get_cache_path(self, domain: str) -> str:
        """Get path for domain-specific cache file."""
        return os.path.join(self.cache_dir, f"{domain.replace('.', '_')}.pkl")

    def _load_cache(self, domain: str) -> Optional[Dict[str, Any]]:
        """Load cached analysis data for domain."""
        cache_path = self._get_cache_path(domain)
        if os.path.exists(cache_path):
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        return None

    def _save_cache(self, domain: str, data: Dict[str, Any]) -> None:
        """Save analysis data to cache."""
        cache_path = self._get_cache_path(domain)
        with open(cache_path, 'wb') as f:
            pickle.dump(data, f)

    async def analyze_website(self, domain: str) -> Dict[str, Any]:
        """
        Perform step-by-step analysis of website structure and keywords.
        
        Args:
            domain: Website domain to analyze
        
        Returns:
            Dict containing analysis results
        """
        self.logger.info(f"Starting analysis for domain: {domain}")
        
        try:
            print("\n" + "="*50)
            print(f"ðŸ” Starting analysis for domain: {domain}")
            print("="*50)
            
            # Step 1: Initialize and validate domain
            print("\nðŸ“Š Step 1: Initializing analysis...")
            if not domain:
                raise ValueError("Domain cannot be empty")
            
            # Step 2: Website Structure Analysis
            print("\nðŸŒ Step 2: Analyzing website structure...")
            structure_analysis = await self._analyze_structure(domain)
            if not structure_analysis:
                raise ValueError("Failed to analyze website structure")
            
            # Step 3: Keyword Research
            print("\nðŸ” Step 3: Performing keyword research...")
            keyword_analysis = await self._analyze_keywords(domain)
            if not keyword_analysis:
                raise ValueError("Failed to perform keyword research")
            
            # Step 4: Content Analysis
            print("\nðŸ“ Step 4: Analyzing existing content...")
            content_analysis = await self._analyze_content(domain)
            
            # Step 5: Competitor Analysis
            print("\nðŸ¢ Step 5: Analyzing competitors...")
            competitor_analysis = await self._analyze_competitors(domain, keyword_analysis)
            
            # Combine all analyses
            analysis_results = {
                'structure': structure_analysis,
                'keywords': keyword_analysis,
                'content': content_analysis,
                'competitors': competitor_analysis,
                'timestamp': datetime.now().isoformat()
            }
            
            # Cache the results
            self._save_cache(domain, analysis_results)
            
            self.logger.info("Analysis completed successfully")
            print("\nâœ… Analysis completed successfully!")
            
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"Error analyzing website: {str(e)}")
            self.logger.error("Stack trace:", exc_info=True)
            raise

    async def _analyze_structure(self, domain: str) -> Dict[str, Any]:
        """Analyze website structure."""
        try:
            self.logger.info("Initializing crawler...")
            self.crawler = WebsiteCrawler(domain)
            
            self.logger.info("Starting crawl...")
            await self.crawler.crawl_site()
            
            self.logger.info("Analyzing crawled content...")
            analysis = self.crawler.analyze_content()
            self.logger.info(f"Analysis: {analysis}")
            if not analysis:
                raise ValueError("No structure data available")
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Structure analysis failed: {str(e)}")
            return self.crawler._get_empty_analysis() if self.crawler else {}

    async def _analyze_keywords(self, domain: str) -> Dict[str, Any]:
        """Perform keyword research."""
        try:
            print(f"ðŸ” Researching keywords for {domain}...")
            keywords = await self.keyword_research.get_keywords(domain)
            
            if not keywords:
                print("âš ï¸ No keywords found, using alternative research method...")
                keywords = await self.keyword_research.get_alternative_keywords(domain)
            
            if keywords:
                selected = await self.keyword_research.select_target_keywords(
                    keywords,
                    min_volume=300,
                    max_difficulty=0.7
                )
                
                return {
                    "total_keywords": len(selected),
                    "selected_keywords": selected
                }
            return {}
            
        except Exception as e:
            self.logger.error(f"Keyword analysis failed: {str(e)}")
            return {}

    async def _analyze_content(self, domain: str) -> Dict[str, Any]:
        """Analyze existing content."""
        try:
            if not self.crawler or not self.crawler.content_data:
                return {}
            
            content_analysis = {
                'total_pages': len(self.crawler.content_data),
                'avg_word_count': 0,
                'content_issues': []
            }
            
            total_words = 0
            for url, content in self.crawler.content_data.items():
                word_count = len(content.split())
                total_words += word_count
                
                # Check content against SEO rules
                issues = self.seo_optimizer.apply_rules(content)
                if issues:
                    content_analysis['content_issues'].append({
                        'url': url,
                        'issues': issues
                    })
                
            if content_analysis['total_pages'] > 0:
                content_analysis['avg_word_count'] = total_words / content_analysis['total_pages']
            
            return content_analysis
            
        except Exception as e:
            self.logger.error(f"Content analysis failed: {str(e)}")
            return {}

    async def _analyze_competitors(self, domain: str, keyword_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze competitors for top keywords."""
        try:
            competitor_data = {}
            selected_keywords = keyword_analysis.get('selected_keywords', [])[:5]  # Analyze top 5 keywords
            
            for keyword_data in selected_keywords:
                keyword = keyword_data['keyword']
                analysis = await self.analyze_competitors(domain, keyword)
                if 'error' not in analysis:
                    competitor_data[keyword] = analysis
            
            return competitor_data
            
        except Exception as e:
            self.logger.error(f"Competitor analysis failed: {str(e)}")
            return {}

    async def analyze_competitors(self, domain: str, keyword: str) -> Dict[str, Any]:
        """
        Analyze competitor content for a specific keyword
        """
        try:
            # Get top 10 ranking pages for the keyword
            serp_data = await self.keyword_research.get_serp_data(keyword)
            competitor_analysis = []
            
            for result in serp_data[:10]:
                if domain not in result['url']:  # Skip own domain
                    content = await self.crawler.get_page_content(result['url'])
                    analysis = {
                        'url': result['url'],
                        'position': result['position'],
                        'content_length': len(content.split()) if content else 0,
                        'page_score': await self.seo_optimizer.analyze_page(result['url'])
                    }
                    competitor_analysis.append(analysis)
            
            return {
                'keyword': keyword,
                'competitor_count': len(competitor_analysis),
                'avg_content_length': sum(a['content_length'] for a in competitor_analysis) / len(competitor_analysis) if competitor_analysis else 0,
                'competitors': competitor_analysis
            }
        except Exception as e:
            self.logger.error(f"Error analyzing competitors: {str(e)}")
            return {'error': str(e)}

    async def generate_content_batch(self, domain: str, num_articles: int = 2) -> List[Dict[str, Any]]:
        """
        Generate multiple pieces of content using cached analysis.
        
        Args:
            domain: Website domain
            num_articles: Number of articles to generate
            
        Returns:
            List of generated content with SEO analysis
        """
        print("\n" + "="*50)
        print(f"ðŸ“ Starting batch content generation for {domain}")
        print(f"Target: {num_articles} articles")
        print("="*50)
        
        analysis_data = self._load_cache(domain)
        if not analysis_data:
            print("âŒ No analysis data found. Please run analyze_website first.")
            return []
        
        keywords = analysis_data.get('keywords', {})  # Remove json.loads since it's already a dict
        selected_keywords = keywords.get('selected_keywords', [])[:num_articles]
        
        results = []
        for idx, keyword_data in enumerate(selected_keywords, 1):
            result = await self._generate_single_content(idx, num_articles, keyword_data)
            results.append(result)
                
        print("\n" + "="*50)
        print(f"âœ¨ Batch generation complete! Generated {len(results)} articles")
        print("="*50)
        
        return results

    async def _generate_single_content(self, idx: int, total: int, keyword_data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate and optimize a single piece of content."""
        print(f"\nðŸ“Œ Generating content {idx}/{total}")
        print(f"Keyword: {keyword_data['keyword']}")
        print(f"Search Volume: {keyword_data.get('search_volume', 'N/A')}")
        print(f"Difficulty: {keyword_data.get('difficulty', 'N/A')}")
        
        # Generate content
        print("ðŸ¤– Generating blog post...")
        content = await self.content_generator.generate_blog_post(keyword_data)
        
        # Optimize content
        print("ðŸ” Checking SEO optimization...")
        issues = await self.optimize_content(content)
        
        # Save content
        filename = await self.content_generator.save_as_markdown(content, keyword_data['keyword'])
        print(f"âœ… Content saved to: {filename}")
        
        result = {
            'keyword': keyword_data['keyword'],
            'content': content,
            'issues': issues if issues else []
        }
        
        await self._print_optimization_results(result['issues'])
        return result

    def _print_optimization_results(self, issues: List[str]) -> None:
        """Print SEO optimization results."""
        if issues:
            print("\nâš ï¸ SEO Issues found:")
            for issue in issues:
                print(f"- {issue}")
        else:
            print("âœ… Content is fully optimized")

    def crawl_website(self, domain: str) -> str:
        """Tool method for crawler"""
        try:
            if not self.crawler or self.crawler.domain != domain:
                self.crawler = WebsiteCrawler(domain)
            self.crawler.crawl()
            return json.dumps(self.crawler.analyze_content(), indent=2)
        except Exception as e:
            return json.dumps({"error": str(e)})
        
    def research_keywords(self, domain):
        """Tool method for keyword research"""
        try:
            print(f"\nðŸ” Researching keywords for {domain}...")
            keywords = self.keyword_research.get_keywords(domain)
            if not keywords:
                print("âŒ No keywords found")
                return json.dumps({"error": "No keywords found"})
            
            print(f"ðŸ“Š Found {len(keywords)} initial keywords")
            print("ðŸŽ¯ Selecting target keywords...")
            
            selected = self.keyword_research.select_target_keywords(
                keywords,
                min_volume=300,
                max_difficulty=0.7
            )
            
            intent_groups = defaultdict(list)
            for kw in selected:
                intent_groups[kw['intent']].append(kw)
            
            print("\nðŸ“ˆ Keyword Analysis Summary:")
            print(f"Selected Keywords: {len(selected)}")
            for intent, kws in intent_groups.items():
                print(f"{intent.capitalize()}: {len(kws)} keywords")
            
            result = {
                "total_keywords": len(selected),
                "keywords_by_intent": {
                    intent: len(kws) for intent, kws in intent_groups.items()
                },
                "selected_keywords": selected
            }
            
            return json.dumps(result, indent=2)
        except Exception as e:
            print(f"âŒ Error in keyword research: {str(e)}")
            return json.dumps({"error": str(e)})
            
    def generate_content(self, keyword_data):
        """Tool method for content generation"""
        content = self.content_generator.generate_blog_post(keyword_data)
        filename = self.content_generator.save_as_markdown(content, keyword_data['keyword'])
        print(f"Blog saved as: {filename}")
        return content
        
    def optimize_content(self, content):
        """Tool method for content optimization"""
        return json.dumps(self.seo_optimizer.apply_rules(content), indent=2)
        
    def track_content_performance(self, domain: str, days: int = 30) -> Dict[str, Any]:
        """
        Track performance of generated content
        """
        analysis_data = self._load_cache(domain)
        if not analysis_data:
            raise ValueError("No analysis data found. Run analyze_website first.")
            
        # Get all generated content
        blog_dir = "blogs"
        performance_data = defaultdict(list)
        
        for keyword_dir in os.listdir(blog_dir):
            keyword_path = os.path.join(blog_dir, keyword_dir)
            if os.path.isdir(keyword_path):
                versions = sorted([f for f in os.listdir(keyword_path) if f.endswith('.md')])
                if versions:
                    latest_version = versions[-1]
                    with open(os.path.join(keyword_path, latest_version)) as f:
                        content = f.read()
                        
                    # Get current keyword position
                    try:
                        new_position = self.keyword_research.get_keyword_position(domain, keyword_dir.replace('_', ' '))
                        performance_data[keyword_dir].append({
                            'version': latest_version,
                            'current_position': new_position,
                            'improvement': analysis_data.get('original_position', 0) - new_position
                        })
                    except Exception as e:
                        self.logger.error(f"Error tracking {keyword_dir}: {str(e)}")
                        
        return dict(performance_data) 